{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# monitoring\n",
    "import time\n",
    "\n",
    "# data cleaning\n",
    "import re\n",
    "\n",
    "# lemmatisation\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# intermediate storage\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# stats\n",
    "import numpy as np\n",
    "\n",
    "# stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Clustering\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# Machine learning\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib.legend_handler import HandlerLine2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_length = 2\n",
    "min_yearly_df = 5   \n",
    "\n",
    "long_ma_length = 7   # 12\n",
    "short_ma_length = 4  #6\n",
    "signal_line_ma = 2   #3\n",
    "significance_ma_length = 2  # actual 3\n",
    "\n",
    "significance_threshold = 0.0002 # actual 0.0002\n",
    "years_above_significance = 3\n",
    "testing_period = 3\n",
    "\n",
    "# Detection threshold is set such that the top 500 terms are chosen\n",
    "burstiness_threshold_prediction = 0.003\n",
    "burstiness_threshold_detection = 0.000020\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 11\n",
    "plt.rcParams['ytick.labelsize'] = 11\n",
    "plt.rc('font', family='sans-serif')\n",
    "\n",
    "year_range = list(range(2014,2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "htmltags = '<[^>]+>'\n",
    "htmlspecial = '&#?[xX]?[a-zA-Z0-9]{2,8};'\n",
    "\n",
    "start_delimiter = 'documentstart'\n",
    "\n",
    "sent_delimiter = 'sentenceboundary'\n",
    "end_delimiter = 'documentend'\n",
    "\n",
    "delimiters = [start_delimiter, sent_delimiter, end_delimiter]\n",
    "\n",
    "# Download the lemmatisesr\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Create a tokeniser\n",
    "count = CountVectorizer(strip_accents='ascii', min_df=1)\n",
    "tokeniser = count.build_analyzer()\n",
    "\n",
    "def normalise_acronymns(text):\n",
    "    '''\n",
    "    Remove the periods in acronyms. \n",
    "    Adapted from the method found at https://stackoverflow.com/a/40197005 \n",
    "    '''\n",
    "    return re.sub(r'(?<!\\w)([A-Z, a-z])\\.', r'\\1', text)\n",
    "\n",
    "def normalise_decimals(text):\n",
    "    '''\n",
    "    Remove the periods in decimal numbers and replace with POINT\n",
    "    '''\n",
    "    return re.sub(r'([0-9])\\.([0-9])', r'\\1POINT\\2', text)\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    \n",
    "    # my addition\n",
    "    text = re.sub(htmltags, \" \", text)\n",
    "    text = re.sub(htmlspecial, \" \", text)\n",
    "    \n",
    "    if \"Ph.D\" in text: \n",
    "        text = text.replace(\"Ph.D.\",\"PhD\")\n",
    "        \n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1\",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1\\\\2\\\\3\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1\\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1 \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1\",text)\n",
    "    \n",
    "    if \"”\" in text: \n",
    "        text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: \n",
    "        text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: \n",
    "        text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: \n",
    "        text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "        \n",
    "    text = text.replace(\".\",\"<stop>\")\n",
    "    text = text.replace(\"?\",\"<stop>\")\n",
    "    text = text.replace(\"!\",\"<stop>\")\n",
    "    \n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    \n",
    "    non_empty = []\n",
    "    for s in sentences: \n",
    "         # we require that there be two alphanumeric characters in a row\n",
    "        if len(re.findall(\"[A-Za-z0-9][A-Za-z0-9]\", s)) > 0:\n",
    "            non_empty.append(s)\n",
    "    return non_empty\n",
    "\n",
    "def pad_sentences(sentences):\n",
    "    '''\n",
    "    Takes a list of sentences and returns a string in which:\n",
    "        - The beginning of the abstract is indicated by DOCUMENTSTART\n",
    "        - The end is indicated by DOCUMENTEND\n",
    "        - Sentence boundaries are indicated by SENTENCEBOUNDARY\n",
    "        \n",
    "    The number of delimiters used is dependent on the ngram length\n",
    "    '''\n",
    "    sent_string = (' '+(sent_delimiter+' ')*(ngram_length-1)).join(sentences)\n",
    "    \n",
    "    return (start_delimiter+' ')*(ngram_length-1) + sent_string + (' '+end_delimiter)*(ngram_length-1)\n",
    "    \n",
    "def cleaning_pipeline(title, abstract):\n",
    "    '''\n",
    "    Takes a binary string and returns a list of cleaned sentences, stripped of punctuation and lemmatised\n",
    "    '''\n",
    "\n",
    "    title = normalise_decimals(normalise_acronymns(title.decode()))\n",
    "    abstract = normalise_decimals(normalise_acronymns(abstract.decode()))\n",
    "    sentences = [title] + split_into_sentences(abstract)\n",
    "    \n",
    "    # strip out punctuation and make lowercase\n",
    "    clean_sentences = []\n",
    "    for s in sentences:\n",
    "        \n",
    "        # Deal with special cases\n",
    "        s = re.sub(r'[-/]', ' ', s)\n",
    "        \n",
    "        # Remove all other punctuation\n",
    "        s = re.sub(r'[^\\w\\s]','',s)\n",
    "        clean_sentences.append(s.lower())\n",
    "        \n",
    "    # pad sentences with delimiters\n",
    "    text = pad_sentences(clean_sentences)\n",
    "    \n",
    "    # Lemmatise word by word\n",
    "    lemmas = []\n",
    "    for word in tokeniser(text):\n",
    "        lemmas.append(wnl.lemmatize(word))\n",
    "        \n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "\n",
    "def calc_macd(dataset):\n",
    "    long_ma = dataset.ewm(span=long_ma_length).mean()\n",
    "    short_ma = dataset.ewm(span=short_ma_length).mean()\n",
    "    significance_ma = dataset.ewm(span=significance_ma_length).mean()\n",
    "    macd = short_ma - long_ma\n",
    "    signal = macd.ewm(span=signal_line_ma).mean()\n",
    "    hist = macd - signal\n",
    "    return long_ma, short_ma, significance_ma, macd, signal, hist\n",
    "\n",
    "def calc_significance(stacked_vectors, significance_threshold, n):\n",
    "    # Must have been above the significance threshold for two consecutive timesteps\n",
    "    a = stacked_vectors > significance_threshold   \n",
    "    b = a.rolling(window = n).sum()\n",
    "    return stacked_vectors[stacked_vectors.axes[1][np.where(b.max()>=n)[0]]]\n",
    "    \n",
    "def calc_burstiness(hist, scaling_factor):\n",
    "    return hist.iloc[long_ma_length-1:]/scaling_factor\n",
    "\n",
    "def calc_scaling(significance_ma, method):\n",
    "    if method == \"max\":\n",
    "        scaling = significance_ma.iloc[significance_ma_length-1:].max()\n",
    "    elif method == \"mean\":\n",
    "        scaling = significance_ma.iloc[significance_ma_length-1:].mean()\n",
    "    elif method == \"sqrt\":\n",
    "        scaling = np.sqrt(significance_ma.iloc[significance_ma_length-1:].max()  )      \n",
    "    return scaling\n",
    "def max_burstiness(burstiness, absolute=False):\n",
    "    if absolute:\n",
    "        b = pd.concat([np.abs(burstiness).max(), burstiness.idxmax()], axis=1)# actual axis = 1\n",
    "    else:\n",
    "        b = pd.concat([burstiness.max(), burstiness.idxmax()], axis=1)\n",
    "    b.columns = [\"max\", \"location\"]\n",
    "    return b\n",
    "\n",
    "def feature_selection(dataset):\n",
    "    '''\n",
    "    Compile the features for the prediction step\n",
    "    '''\n",
    "    long_ma = dataset.ewm(span=long_ma_length).mean()\n",
    "    short_ma = dataset.ewm(span=short_ma_length).mean()\n",
    "    significance_ma = dataset.ewm(span=significance_ma_length).mean()\n",
    "    macd = short_ma - long_ma\n",
    "    signal = macd.ewm(span=signal_line_ma).mean()\n",
    "    hist = macd - signal\n",
    "    \n",
    "    scaling_factor = calc_scaling(significance_ma, \"sqrt\")\n",
    "    burstiness_over_time = calc_burstiness(hist, scaling_factor)\n",
    "    burstiness = max_burstiness(burstiness_over_time)\n",
    "    \n",
    "    \n",
    "    X = long_ma.iloc[long_ma_length:].T\n",
    "    scaled_hist = hist.iloc[long_ma_length:]/scaling_factor\n",
    "    scaled_signal = signal.iloc[long_ma_length:]/scaling_factor\n",
    "    \n",
    "    Xtra = pd.concat([significance_ma.iloc[-1], \n",
    "                      dataset.iloc[-1],\n",
    "                        significance_ma.iloc[significance_ma_length:].std()/scaling_factor,\n",
    "                        significance_ma.iloc[significance_ma_length:].max(),\n",
    "                        significance_ma.iloc[significance_ma_length:].min(),\n",
    "                      scaling_factor\n",
    "                        ], axis=1)\n",
    "    X = pd.concat([X,scaled_hist.T,scaled_signal.T,Xtra], axis=1)\n",
    "\n",
    "    X.columns = [str(i) for i in range(8)] + [\"hist\"+str(i) for i in range(8)] + [\"signal\"+str(i) for i in range(8)] + [\n",
    "          \"significance\",\n",
    "                        \"prevalence\",\n",
    "                        \"scaled std\",\n",
    "                        \"max\",\n",
    "                        \"min\",\n",
    "                        \"scaling\"\n",
    "                    ]\n",
    "\n",
    "\n",
    "    return X\n",
    "\n",
    "def balanced_subsample(x,y,subsample_size=1.0):\n",
    "    # from https://stackoverflow.com/a/23479973\n",
    "    class_xs = []\n",
    "    min_elems = None\n",
    "\n",
    "    for yi in np.unique(y):\n",
    "        elems = x[(y == yi)]\n",
    "        class_xs.append((yi, elems))\n",
    "        if min_elems == None or elems.shape[0] < min_elems:\n",
    "            min_elems = elems.shape[0]\n",
    "\n",
    "    use_elems = min_elems\n",
    "    if subsample_size < 1:\n",
    "        use_elems = int(min_elems*subsample_size)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for ci,this_xs in class_xs:\n",
    "        if len(this_xs) > use_elems:\n",
    "            np.random.shuffle(this_xs)\n",
    "\n",
    "        x_ = this_xs[:use_elems]\n",
    "        y_ = np.empty(use_elems)\n",
    "        y_.fill(ci)\n",
    "        xs.append(x_)\n",
    "        ys.append(y_)\n",
    "\n",
    "    xs = np.concatenate(xs)\n",
    "    ys = np.concatenate(ys)\n",
    "\n",
    "    return xs,ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"C:/drive/new/Institute/ACADEMY OF SCIENTIFIC AND INNOVATIVE RESEARCH ACSIR.csv\",encoding='latin-1')\n",
    "column = ['PY','SC','WC','DE']\n",
    "df = pd.read_csv(\"C:/drive/all_merge.csv\", usecols = column,low_memory=True)\n",
    "df1 = df[['PY','WC']]\n",
    "df1.to_csv(\"C:/drive/mergenew.csv\")\n",
    "df1 = df1.dropna()\n",
    "year = [2014,2015,2016,2017,2018]\n",
    "df1 = df1[df1['PY'].isin(year)] \n",
    "df1.shape\n",
    "print(df['PY'].unique())\n",
    "# df1 = df[['PD','PT']] (correct but not used right now)\n",
    "df1.groupby(df1['PY']).size().plot(kind='bar')\n",
    "#df1.groupby(df1['PD']).size().plot(kind='bar') # wrong data in file PY saved as PD\n",
    "plt.title(\" Documnets per Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of documnets\")\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average length of titles and abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "dftext = df[['TI','AB','ID','DE']]\n",
    "#dftext['TI'].value_counts()[:30].plot(kind='barh')\n",
    "dftext['TI'].value_counts()[:10].plot(kind='barh')\n",
    "dftext['ID'].value_counts()[:10].plot(kind='barh')\n",
    "dftext['AB'].value_counts()[:10].plot(kind='barh')\n",
    "dftext['DE'].value_counts()[:10].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stop = set([s.replace(\"'\", \"\") for s in stop])\n",
    "\n",
    "# Add years to prevent spikes\n",
    "for year in range(2014, 2019):\n",
    "    stop.add(str(year))\n",
    "\n",
    "# Add small numbers\n",
    "for num in range(0, 100):\n",
    "    if len(str(num)) < 2:\n",
    "        stop.add(str(num))\n",
    "        num = '0' + str(num)\n",
    "        \n",
    "    stop.add(str(num))\n",
    "    \n",
    "# Add these extra stopwords to the list\n",
    "extra = [\n",
    "    'use', 'using', 'uses', 'used', 'based', 'including', 'include', 'approach',\n",
    "    'wa', 'ha', 'doe'\n",
    "        ]\n",
    "for word in extra:\n",
    "    stop.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dft = dft.sort_values(['PY','AB'],ascending = True)\n",
    "df1 = df1.sort_values(['PY','DE'],ascending = True) # PY==PD ,AB ==ID\n",
    "\n",
    "#for y in range(1989,2009):\n",
    "for y in range(2014,2018):\n",
    "    #dd = dft.loc[dft['PY'] == y,['PY','TI']] \n",
    "    \n",
    "    dd = df1.loc[df1['PY'] == y,['PY','DE']] \n",
    "    print(dd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a vocabulary\n",
    "\n",
    "We have to build a vocabulary before we vectorise the data. This is because we want to set limits on the size of the vocabulary.\n",
    "\n",
    "Terms must occur at least 5 times in at least one year. This removes one-off spelling errors or excessively rare terms, which, given the intended application, are not interesting to us.\n",
    "We take uni, bi and tri-grams.\n",
    "We use sentence delimeters to avoid taking bi and tri-grams across sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used when three field data is taken \n",
    "cv=CountVectorizer()\n",
    "sample = pd.DataFrame(df1['DE']+','+df1['ID'], columns=['Output'])\n",
    "word_count_vector=cv.fit_transform(sample['Output'])\n",
    "print(word_count_vector.shape)\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "df1 = df1.sort_values(['PY','WC',],ascending = True)\n",
    "for y in range(2014, 2019):\n",
    "    \n",
    "    dd = df1.loc[df1['PY'] == y,['PY','WC']] \n",
    "    #dd = dft.loc[dft['PD'] == y,['PD','CA']] \n",
    "    #sample = pd.DataFrame(df1['DE']+','+df1['ID']+','+df1['TI'], columns=['Output'])\n",
    "    \n",
    "    t0 = time.time()\n",
    "   \n",
    " #     vectorizer = CountVectorizer(strip_accents='ascii', \n",
    "#                                   stop_words='english',\n",
    "#                                  ngram_range=(1,3), \n",
    "#                                  min_df=5,  max_features=20000)\n",
    "    vectorizer = CountVectorizer(strip_accents ='ascii', \n",
    "                                 stop_words='english',\n",
    "                                 ngram_range=(1,3), \n",
    "                                 min_df=30,max_features=200000)  \n",
    "#    vector = vectorizer.fit_transform(sample['Output'])# .values.settype('U') for this we convert to unicode if not \n",
    "    vector = vectorizer.fit_transform(dd.WC)                                           # this can run without it \n",
    "    #print(vector.toarray())\n",
    "    # Save the new words\n",
    "    vocab = vocab.union(vectorizer.vocabulary_.keys())\n",
    "    #print(vocab)\n",
    "    print(y, len(vocab), time.time()-t0)\n",
    "\n",
    "vocabulary = {}\n",
    "i = 0\n",
    "for v in vocab:\n",
    "    # Remove delimiters\n",
    "    if start_delimiter in v:\n",
    "        pass\n",
    "    elif end_delimiter in v:\n",
    "        pass\n",
    "    elif sent_delimiter in v:\n",
    "        pass\n",
    "    else:\n",
    "        vocabulary[v] = i\n",
    "        i += 1\n",
    "        \n",
    "print(len(vocabulary.keys()))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go year by year and vectorise based on our vocabulary\n",
    "We read in the cleaned data and vectorise it according to our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "df1 = df1.sort_values(['PY','WC'],ascending = True) # PY==PD ,AB ==ID ,TI ==CA\n",
    "\n",
    "#for y in range(1989,2009):\n",
    "for y in range(2014,2019):\n",
    "    dd = df1.loc[df1['PY'] == y,['PY','WC']] \n",
    "    #dd = df1.loc[df1['PY'] == y,['PD','CA']] \n",
    "    \n",
    "    # The same as above, applied year by year instead.\n",
    "    t0 = time.time()\n",
    "\n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='ascii', \n",
    "                                ngram_range=(1,3),\n",
    "                                stop_words='english',\n",
    "                                vocabulary=vocabulary)\n",
    "                                \n",
    "    #vectorizer = CountVectorizer(ngram =(1,3) ,stop_words = stop ,vocabulary = vocabulary)\n",
    "  \n",
    "    vectors.append(vectorizer.fit_transform(dd.WC))\n",
    "    #print(vectors)\n",
    "    \n",
    "    print(y, time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_vectors = []\n",
    "for y in range(len(vectors)):\n",
    "    \n",
    "    vector = vectors[y]\n",
    "    \n",
    "    # Set all elements that are greater than one to one -- we do not care if a word is used multiple times in \n",
    "    # the same document\n",
    "    vector[vector>1] = 1\n",
    "\n",
    "    # Sum the vector along columns\n",
    "    summed = np.squeeze(np.asarray(np.sum(vector, axis=0)))\n",
    "   \n",
    "    normalised = summed/vector.shape[0]\n",
    "    summed_vectors.append(normalised)\n",
    "print(\"stacked_transpose\")    \n",
    "stacked_vectors = np.stack(summed_vectors, axis=1)\n",
    "print(stacked_vectors)\n",
    "print(stacked_vectors.transpose().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing the vectors\n",
    "We sum the vectors along columns, so that we have the popularity of each term in each year.\n",
    "\n",
    "All >1 elements are set to 1. This is because it does not matter for our application if a word is used multiple times in an abstract.\n",
    "We divide by the number of documents in each year to normalise the score. Therefore, the 1988 column is divided by ~6000, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_vectors = []\n",
    "for y in range(len(vectors)):\n",
    "    \n",
    "    vector = vectors[y]\n",
    "    \n",
    "    # Set all elements that are greater than one to one -- we do not care if a word is used multiple times in \n",
    "    # the same document\n",
    "    #vector[vector>1] = 1\n",
    "\n",
    "    # Sum the vector along columns\n",
    "    summed = np.squeeze(np.asarray(np.sum(vector, axis=0)))\n",
    "    \n",
    "    # Normalise by dividing by the number of documents in that year\n",
    "    normalised = summed/vector.shape[0]\n",
    "    \n",
    "    # Save the summed vector\n",
    "    summed_vectors.append(normalised)\n",
    "    \n",
    "     #stack vectors vertically, so that we have the full history of popularity/time for each term\n",
    "stacked_vectors = np.stack(summed_vectors, axis=1)\n",
    "\n",
    "#stacked_vectors = np.stack(summed_vectors)\n",
    "print(stacked_vectors.shape)\n",
    "\n",
    "stacked_vectors=pd.DataFrame(stacked_vectors.transpose(), columns=list(vocabulary.keys()))\n",
    "print(stacked_vectors)\n",
    "#stacked_vectors.to_csv(\"C:\\drive\\stacked_vectors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectors, open('C:/drive/burst/stacked_vectors.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorise again, using these terms only for the bursty vectors\n",
    "vectors = []\n",
    "for year in range(2014, 2019):\n",
    "\n",
    "# The same as above, applied year by year instead.\n",
    "    t0 = time.time()\n",
    "    vectorizer = CountVectorizer(strip_accents='ascii',ngram_range=(1,3),\n",
    "                                  stop_words='english', vocabulary=bursts)\n",
    "    vector = vectorizer.fit_transform(df1.WC)\n",
    "# If any element is larger than one, set it to one\n",
    "    vector.data = np.where(vector.data>0, 1, 0)\n",
    "    vectors.append(vector)\n",
    "pickle.dump(vectors, open('C:/drive/burst/burstvectors_500.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_vectors = pickle.load(open('C:/drive/burst/stacked_vectors.p', \"rb\"))\n",
    "burstvectors = pickle.load(open('C:/drive/burst/burstvectors_500.p', \"rb\"))\n",
    "bursts = pickle.load(open('C:/drive/burst/bursts.p', \"rb\"))\n",
    "clusters = pickle.load(open('C:/drive/burst/clusters.p', \"rb\"))\n",
    "stackedvectors = pickle.load(open('C:/drive/burst/stackedvectors.p', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stackedvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_vectors1 = pd.DataFrame(stackedvectors)\n",
    "stacked_vectors1.to_csv(\"C:/drive/burst/stacked_vectorsfinal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_list = []\n",
    "for list in stacked_vectors:\n",
    "    ml_list.append(list)\n",
    "stacked_vectors2 = pd.DataFrame(ml_list)\n",
    "stacked_vectors2.to_csv(\"C:/drive/burst/stacked_vectors3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_list = []\n",
    "for list in stackedvectors:\n",
    "    input()\n",
    "    print(list)\n",
    "    for l in list:\n",
    "        print(l)\n",
    "        ml_list.append(l)\n",
    "    \n",
    "# stacked_vectors1 = pd.DataFrame(stacked_vectors)\n",
    "# stacked_vectors1.to_csv(\"C:/drive/burst/stacked_vectorsnew.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in stacked_vectors : \n",
    "    print(row) \n",
    "rez = [[stacked_vectors[j][i] for j in range(len(stacked_vectors))] for i in range(len(stacked_vectors[0]))] \n",
    "print(\"\\n\") \n",
    "for row in rez: \n",
    "    print(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise the vectors again\n",
    "The number of tokens per abstract has changed over time. We now normalise again, so each year sums to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not required in my case as I am not using Abstract NO RRRUn\n",
    "normalisation = stacked_vectors.sum(axis=1)\n",
    "print(normalisation)\n",
    "stacked_vectors = stacked_vectors.divide(normalisation, axis='index')*100\n",
    "print(stacked_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply a significance threshold\n",
    "We require that each term has been above a given significance threshold for 3 years. This shortens the vocabulary and \n",
    "removes single year spikes due to anomalous events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used in my case no RRUN\n",
    "stacked_vectors = calc_significance(stacked_vectors, significance_threshold, years_above_significance)\n",
    "print(stacked_vectors.shape)\n",
    "print(stacked_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectors, open('C:/drive/burst/stackedvectors.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_ma, short_ma, significance_ma, macd, signal, hist = calc_macd(stacked_vectors)\n",
    "scaling_factor = calc_scaling(significance_ma, \"max\")\n",
    "# print(scaling_factor)\n",
    "burstiness_over_time = calc_burstiness(hist,False)\n",
    "print(burstiness_over_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate burstiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_ma, short_ma, significance_ma, macd, signal, hist = calc_macd(stacked_vectors)\n",
    "\n",
    "# scaling_factor = calc_scaling(significance_ma, \"max\")\n",
    "# print(scaling_factor)\n",
    "# burstiness_over_time = calc_burstiness(hist, scaling_factor)\n",
    "burstiness_over_time = hist\n",
    "\n",
    "#burstiness_over_time = calc_burstiness(hist, inverse)\n",
    "# print(\"burstiness_over_time:\")\n",
    "# print(burstiness_over_time.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(burstiness_over_time[b])\n",
    "burstiness = max_burstiness(burstiness_over_time,True)\n",
    "#burstiness = abs(burstiness_over_time)\n",
    "print(burstiness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a threshold such that the top 500 bursty terms are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(np.sum(burstiness[\"max\"]>0.000024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bursts = list(burstiness[\"max\"].index[np.where(burstiness[\"max\"]>burstiness_threshold_detection)[0]])\n",
    "bursts = list(burstiness[\"max\"].index[np.where(burstiness[\"max\"]>0.000024)[0]])\n",
    "print(bursts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bursts, open('C:/drive/burst/bursts.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursts1 = pd.DataFrame(bursts)\n",
    "bursts1.to_csv(\"C:/drive/burst/bursts2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Based on Co-occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorise again, using these terms only\n",
    "vectors = []\n",
    "df1 = df1.sort_values(['PY','WC'],ascending = True) # PY==PD ,AB ==ID ,TI ==CA\n",
    "\n",
    "#for y in range(1989,2009):\n",
    "for y in range(2014,2019):\n",
    "    dd = df1.loc[df1['PY'] == y,['PY','WC']] \n",
    "    #dd = df1.loc[df1['PY'] == y,['PD','CA']] \n",
    "    \n",
    "    # The same as above, applied year by year instead.\n",
    "    t0 = time.time()\n",
    "\n",
    "\n",
    "    vectorizer = CountVectorizer(strip_accents='ascii', \n",
    "                                ngram_range=(1,3),\n",
    "                                stop_words='english',\n",
    "                                vocabulary=vocabulary)\n",
    "                                \n",
    "\n",
    "    vector = vectorizer.fit_transform(dd.WC)\n",
    "    \n",
    "    # If any element is larger than one, set it to one\n",
    "    vector.data = np.where(vector.data>0, 1, 0)\n",
    "    \n",
    "    vectors.append(vector)\n",
    "    \n",
    "    print(y, time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "cooccurrence = []\n",
    "for v in vectors:\n",
    "    i = i+1\n",
    "    c = v.T*v\n",
    "    c.setdiag(0)\n",
    "    c = c.todense()\n",
    "    cooccurrence.append(c)\n",
    "all_cooccurrence = np.sum(cooccurrence, axis=0)\n",
    "\n",
    "# Translate co-occurence into a distance\n",
    "dists = 1- all_cooccurrence/all_cooccurrence.max()\n",
    "\n",
    "# Remove the diagonal (squareform requires diagonals be zero)\n",
    "dists -= np.diag(np.diagonal(dists))\n",
    "\n",
    "# Put the distance matrix into the fṁormat required by hierachy.linkage\n",
    "flat_dists = squareform(dists)\n",
    "\n",
    "# Get the linkage matrix\n",
    "linkage_matrix = hierarchy.linkage(flat_dists, \"ward\")\n",
    "\n",
    "assignments = hierarchy.fcluster(linkage_matrix, 120, 'maxclust')\n",
    "\n",
    "print(len(bursts))\n",
    "print(len(set(assignments)))\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "\n",
    "for term, assign in zip(bursts, assignments):\n",
    "    clusters[assign].append(term)\n",
    "    \n",
    "print(clusters)\n",
    "\n",
    "for key in sorted(clusters.keys()):\n",
    "    print(key, ':',  ', '.join(clusters[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clusters, open('C:/drive/burst/clusters.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = pd.DataFrame(clusters)\n",
    "cluster.to_csv('C:/drive/burst/clusters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph selected bursty terms over time\n",
    "We manually remove clusters that contain copyright declarations, etc. Then we filter down to 52, choosing \n",
    "a representative sample over time. We choose one or two terms to represent each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('C:\\\\drive\\\\burst\\\\bursts.csv')\n",
    "df1.head(5)\n",
    "clusters = [d.split(', ') for d in df1['terms value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectors, open('C:/drive/burst/bursts.p', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prevalence(cluster):\n",
    "    indices = []\n",
    "    for term in cluster:\n",
    "        indices.append(bursts.index(term))\n",
    "        print(indices)\n",
    "    prevalence = []\n",
    "    for year in range(5):\n",
    "        prevalence.append(100*np.sum(np.sum(stacked_vectors[year][:,indices], axis=1)>0)/stacked_vectors[year].shape[0])\n",
    "        print(prevalence)\n",
    "    return prevalence\n",
    "\n",
    "\n",
    "yplots = 13\n",
    "xplots = 4\n",
    "fig, axs = plt.subplots(yplots, xplots)\n",
    "plt.subplots_adjust(right=1, hspace=0.9, wspace=0.3)\n",
    "plt.suptitle('Prevalence of selected bursty clusters over time', fontsize=14)\n",
    "fig.subplots_adjust(top=0.95)\n",
    "fig.set_figheight(16)\n",
    "fig.set_figwidth(12)\n",
    "x = np.arange(0,5)\n",
    "prevalences = []\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(cluster)\n",
    "    prevalence = get_prevalence(cluster)\n",
    "    prevalences.append(prevalence)\n",
    "#     title = df1.name[i]\n",
    "    title = df1['name'][i]\n",
    "    axs[int(np.floor((i/xplots)%yplots)), i%xplots].plot(x, prevalence, color='k', ls='-', label=title)\n",
    "    axs[int(np.floor((i/xplots)%yplots)), i%xplots].grid()\n",
    "    ymax=np.ceil(max(prevalence)*2)/2\n",
    "    if ymax == 0.5 and max(prevalence) <0.25:\n",
    "        ymax=0.25\n",
    "    elif ymax == 2.5:\n",
    "        ymax=3\n",
    "    axs[int(np.floor((i/xplots)%yplots)), i%xplots].set_ylim(0,ymax)\n",
    "    axs[int(np.floor((i/xplots)%yplots)), i%xplots].set_xlim(0,30)\n",
    "    axs[int(np.floor((i/xplots)%yplots)), i%xplots].set_title(title, fontsize=12)\n",
    "    \n",
    "    \n",
    "    if i%yplots != yplots-1:\n",
    "        axs[i%yplots, int(np.floor((i/yplots)%xplots))].set_xticklabels([])\n",
    "    else:\n",
    "        axs[i%yplots, int(np.floor((i/yplots)%xplots))].set_xticklabels([1988, 1998, 2008, 2018])\n",
    "        \n",
    "axs[6,0].set_ylabel('Percentage of documents containing term (%)', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_data = {}\n",
    "year_range = [2014,2015,2016,2017,2018]\n",
    "for year in range(2014, 2019):\n",
    "    year_idx = year_range.index(year)\n",
    "    print(year_idx)\n",
    "    # Use our three-year method to calc significance\n",
    "    valid_vectors = calc_significance(stacked_vectors[:year_idx], significance_threshold, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_data = {}\n",
    "year_range = [2014,2015,2016,2017,2018]\n",
    "for year in range(2014, 2019):\n",
    "    year_idx = year_range.index(year)\n",
    "    burstsnew = stackedvectors.keys()[bursts]\n",
    "    \n",
    "    # Create a new, much smaller dataset\n",
    "    dataset = stackedvectors[burstsnew].iloc[:year_idx+1]\n",
    "    \n",
    "    # Get the scaled y values\n",
    "    if year < 2018:\n",
    "        y = stackedvectors[bursts].iloc[year_idx]\n",
    "# Select features and store the data\n",
    "    development_data[year] = {}\n",
    "    development_data[year][\"X\"] = feature_selection(dataset)\n",
    "    if year < 2018:\n",
    "        development_data[year][\"y\"]=y-development_data[year][\"X\"]['significance']\n",
    "    print(year, len(bursts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # Recalculate the macd things based on this more limited dataset\n",
    "    long_ma, short_ma, significance_ma, macd, signal, hist = calc_macd(stacked_vectors)\n",
    "    \n",
    "    \n",
    "    # Calculate scaling factor\n",
    "    scaling_factor = calc_scaling(significance_ma.iloc[max(long_ma_length, year_idx-19):year_idx+1], \"sqrt\")\n",
    "\n",
    "    # Calculate the burstiness\n",
    "    burstiness_over_time = calc_burstiness(hist, scaling_factor)\n",
    "    burstiness = max_burstiness(burstiness_over_time)\n",
    "\n",
    "    # Choose terms that are above both thresholds (burstiness, and also most recent year was significant)\n",
    "    burst_idx = np.where((burstiness[\"max\"]>0.000024)&(significance_ma.iloc[year_idx]>significance_threshold))[0]\n",
    "    \n",
    "    # Find the actual names of these terms\n",
    "    bursts = valid_vectors.keys()[burst_idx]\n",
    "    \n",
    "    # Create a new, much smaller dataset\n",
    "    dataset = stacked_vectors[bursts].iloc[:year_idx+1]\n",
    "    \n",
    "    # Get the scaled y values\n",
    "    if year < 2018:\n",
    "        y = stacked_vectors[bursts].iloc[year_idx]\n",
    "# Select features and store the data\n",
    "    development_data[year] = {}\n",
    "    development_data[year][\"X\"] = df1['PY']\n",
    "    if year < 2018:\n",
    "        development_data[year][\"y\"]=y-development_data[year][\"X\"]['WC']\n",
    "    print(year, len(bursts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "\n",
    "for year in stacked_vectors:\n",
    "    for item in year:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    # Get the scaled y values\n",
    "    if year < 2018:\n",
    "        y = stacked_vectors[bursts].iloc[year_idx]\n",
    "# Select features and store the data\n",
    "    development_data[year] = {}\n",
    "    development_data[year][\"X\"] = df1['PY']\n",
    "    if year < 2018:\n",
    "        development_data[year][\"y\"]=y-development_data[year][\"X\"]['WC']\n",
    "    print(year, len(bursts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_data = {}\n",
    "year_range = [2014,2015,2016,2017,2018]\n",
    "for year in range(2014, 2019):\n",
    "    year_idx = year_range.index(year)\n",
    "    \n",
    "    \n",
    "    # Get the scaled y values\n",
    "    if year < 2018:\n",
    "        y = stacked_vectors[bursts].iloc[year_idx]\n",
    "# Select features and store the data\n",
    "    development_data[year] = {}\n",
    "    development_data[year][\"X\"] = df1['PY']\n",
    "    if year < 2018:\n",
    "        development_data[year][\"y\"]=y-development_data[year][\"X\"]['WC']\n",
    "    print(year, len(bursts))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"C:\\drive\\stacked_vectors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "column = ['PY', 'WC']\n",
    "df1 = pd.read_csv(\"C:/drive/all_merge.csv\", usecols = column,low_memory=True)\n",
    "X=df1[['PY']]  # Features\n",
    "y=df1['WC']  # Labels\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for y in range(2014,2019):\n",
    "    dd = df1.loc[df1['PY'] == y,['PY','WC']]     \n",
    "    cv = CountVectorizer(strip_accents ='ascii', \n",
    "                     ngram_range=(1,3),min_df=30,\n",
    "                     max_features=200000)  \n",
    "    count_vector=cv.fit_transform(dd.WC)\n",
    "    vocab = vocab.union(vectorizer.vocabulary_.keys())\n",
    "    #print(vocab)\n",
    "    print(y, len(vocab), time.time()-t0)\n",
    "\n",
    "vocabulary = {}\n",
    "i = 0\n",
    "for v in vocab:\n",
    "    # Remove delimiters\n",
    "    if start_delimiter in v:\n",
    "        pass\n",
    "    elif end_delimiter in v:\n",
    "        pass\n",
    "    elif sent_delimiter in v:\n",
    "        pass\n",
    "    else:\n",
    "        vocabulary[v] = i\n",
    "        i += 1\n",
    "        \n",
    "print(len(vocabulary.keys()))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_data = {}\n",
    "for year in range(2014, 2019):\n",
    "    year_idx = year_range.index(year)\n",
    "    \n",
    "    # Use our three-year method to calc significance\n",
    "    valid_vectors = calc_significance(df2[:year_idx+1], significance_threshold, 1)\n",
    "    \n",
    "    # Recalculate the macd things based on this more limited dataset\n",
    "    long_ma, short_ma, significance_ma, macd, signal, hist = calc_macd(valid_vectors)\n",
    "    \n",
    "    \n",
    "    # Calculate scaling factor\n",
    "    scaling_factor = calc_scaling(significance_ma.iloc[max(long_ma_length, year_idx-19):year_idx+1], \"sqrt\")\n",
    "\n",
    "    # Calculate the burstiness\n",
    "    burstiness_over_time = calc_burstiness(hist, scaling_factor)\n",
    "    burstiness = max_burstiness(burstiness_over_time)\n",
    "\n",
    "    # Choose terms that are above both thresholds (burstiness, and also most recent year was significant)\n",
    "    burst_idx = np.where((burstiness[\"max\"]>0.0012)&(significance_ma.iloc[year_idx]>significance_threshold))[0]\n",
    "    \n",
    "    # Find the actual names of these terms\n",
    "    bursts = valid_vectors.keys()[burst_idx]\n",
    "    \n",
    "    # Create a new, much smaller dataset\n",
    "    dataset = stacked_vectors[bursts].iloc[year_idx-19:year_idx+1]\n",
    "    \n",
    "    # Get the scaled y values\n",
    "    if year < 2015:\n",
    "        y = stacked_vectors[bursts].iloc[year_idx+testing_period]\n",
    "\n",
    "\n",
    "    # Select features and store the data\n",
    "    development_data[year] = {}\n",
    "    development_data[year][\"X\"] = feature_selection(dataset)\n",
    "    if year < 2015:\n",
    "        development_data[year][\"y\"]=y-development_data[year][\"X\"]['significance']\n",
    "    print(year, len(bursts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for threshold in np.arange(0.0006, 0.0017, 0.0002):\n",
    "    scores[threshold] = {}\n",
    "    for year in range(2008, 2013):\n",
    "        year_idx = year_range.index(year)\n",
    "\n",
    "        # Use our three-year method to calc significance\n",
    "        valid_vectors = calc_significance(stacked_vectors[:year_idx+1], significance_threshold, 3)\n",
    "\n",
    "        # Recalculate the macd things based on this more limited dataset\n",
    "        long_ma, short_ma, significance_ma, macd, signal, hist = calc_macd(valid_vectors)\n",
    "\n",
    "\n",
    "        # Calculate scaling factor\n",
    "        scaling_factor = calc_scaling(significance_ma.iloc[max(long_ma_length, year_idx-19):year_idx+1], \"sqrt\")\n",
    "\n",
    "        # Calculate the burstiness\n",
    "        burstiness_over_time = calc_burstiness(hist, scaling_factor)\n",
    "        burstiness = max_burstiness(burstiness_over_time)\n",
    "\n",
    "        # Choose terms that are above both thresholds (burstiness, and also most recent year was significant)\n",
    "        burst_idx = np.where((burstiness[\"max\"]>threshold)&(significance_ma.iloc[year_idx]>significance_threshold))[0]\n",
    "\n",
    "        # Find the actual names of these terms\n",
    "        bursts = valid_vectors.keys()[burst_idx]\n",
    "\n",
    "        # Create a new, much smaller dataset\n",
    "        dataset = stacked_vectors[bursts].iloc[year_idx-19:year_idx+1]\n",
    "\n",
    "        # Select features and store the data\n",
    "        development_data[year] = {}\n",
    "        development_data[year][\"X\"] = feature_selection(dataset)\n",
    "        \n",
    "        development_data[year][\"y\"] = {}\n",
    "        \n",
    "        for interval in range(1,6):\n",
    "            # Get the scaled y values\n",
    "            y = stacked_vectors[bursts].iloc[year_idx+interval]\n",
    "            development_data[year][\"y\"][interval]=y-development_data[year][\"X\"]['significance']\n",
    "    \n",
    "    \n",
    "    X = np.array(pd.concat([development_data[year][\"X\"] for year in range(2008,2013)]))\n",
    "    \n",
    "    for interval in range(1,6):\n",
    "        scores[threshold][interval] = {}\n",
    "        scores[threshold][interval]['scores'] = []\n",
    "        y = np.array(pd.concat([development_data[year][\"y\"][interval] for year in range(2008,2013)]))\n",
    "        \n",
    "        # Binarise y data\n",
    "        y_thresh = np.zeros_like(y)\n",
    "        y_thresh[y>0] = 1\n",
    "        \n",
    "        # Balance the sample\n",
    "        X_bal, y_thresh = balanced_subsample(X, y_thresh,subsample_size=1.0)\n",
    "        \n",
    "        scores[threshold][interval]['size'] = len(y_thresh)\n",
    "        kf = KFold(n_splits=10, shuffle=True)\n",
    "        for train, test in kf.split(X_bal):\n",
    "            clf = RandomForestClassifier(n_estimators=150, max_depth=13)\n",
    "\n",
    "            clf.fit(X_bal[train], y_thresh[train])\n",
    "            preds = clf.predict(X_bal[test])\n",
    "\n",
    "            new_scores = [\n",
    "                sklearn.metrics.accuracy_score(y_thresh[test], preds),\n",
    "                sklearn.metrics.f1_score(y_thresh[test], preds),\n",
    "                np.sum(y_thresh[test]==0)/len(y_thresh[test])\n",
    "            ]\n",
    "            scores[threshold][interval]['scores'].append(new_scores)\n",
    "        \n",
    "        print(threshold, interval, len(y_thresh), np.round(np.mean(np.array(scores[threshold][interval]['scores'])[:,0]),3)\n",
    "             )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in np.arange(0.0006, 0.0017, 0.0002):\n",
    "    print(threshold, '&', \n",
    "          scores[threshold][3]['size'], '&', \n",
    "          np.round(np.mean(np.array(scores[threshold][3]['scores'])[:,0]),2), \n",
    "          '$\\pm$', \n",
    "          np.round(np.std(np.array(scores[threshold][3]['scores'])[:,0]),2), '&', \n",
    "          np.round(np.mean(np.array(scores[threshold][3]['scores'])[:,1]),2), \n",
    "          '$\\pm$', \n",
    "          np.round(np.std(np.array(scores[threshold][3]['scores'])[:,1]),2), \n",
    "          '\\\\\\\\'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', family='sans-serif')\n",
    "plt.rc('xtick', labelsize='medium')\n",
    "plt.rc('ytick', labelsize='medium')\n",
    "line_styles = ['-', '--', ':']\n",
    "col = 0.5\n",
    "fig = plt.figure(figsize=(6,3.7))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_title('Choosing a prediction interval, I', fontsize=13)\n",
    "ax.grid()\n",
    "ax.set_ylim(0.65,0.9)\n",
    "#ax.set_xlim(1,5)\n",
    "\n",
    "ax.set_ylabel('F1 score', fontsize=12)\n",
    "ax.set_xlabel('Number of years in future', fontsize=12)\n",
    "\n",
    "plt.xticks(range(1,6), range(1,6))\n",
    "\n",
    "thresholds = [\"0.0006\", \"0.0008\", \"0.0010\", \"0.0012\", \"0.0014\", \"0.0016\"]\n",
    "for i, threshold in enumerate(np.arange(0.0006, 0.0017, 0.0002)):\n",
    "    y = []\n",
    "    yerr = []\n",
    "    for interval in range(1,6):\n",
    "        s = np.array(scores[threshold][interval]['scores'])\n",
    "        y.append(np.mean(s[:,1]))\n",
    "        yerr.append(np.std(s[:,1]))\n",
    "    \n",
    "    ax.errorbar(range(1,6), y, yerr=yerr, color=str(col),  label=thresholds[i], fmt='--o')\n",
    "        \n",
    "    col-=0.1\n",
    "    \n",
    "#ax.legend(ncol=3, mode=\"expand\")\n",
    "ax.legend(fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>>from sklearn.ensemble import RandomForestClassifier\n",
    ">>>RF_clf = RandomForestClassifier(n_estimators=10)\n",
    ">>>predicted = RF_clf.predict(X_test)\n",
    ">>>print '\\n Here is the classification report:'\n",
    ">>>print classification_report(y_test, predicted)\n",
    ">>>cm = confusion_matrix(y_test, y_pred)\n",
    ">>>print cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_clf = RandomForestClassifier(n_estimators=10)\n",
    "predicted = RF_clf.predict(X_test)\n",
    "print '\\n Here is the classification report:'\n",
    "print classification_report(y_test, predicted)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
